{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"<p>Nextflow Tower is the centralized command-post for the management of Nextflow data pipelines. It brings monitoring, logging &amp; observability, to distributed workflows and simplifies the deployment of pipelines on any cloud, cluster or laptop.</p> <p>Users can launch pre-configured pipelines with ease while the flexible API provides programmatic integration to meet the needs of organizations building on top of Nextflow Tower. Workflow developers can publish pipelines to shared workspaces and administrators can set up and manage the infrastructure required to run data analysis at scale.</p> <p></p>","title":"Home"},{"location":"#what-is-nextflow","text":"<p>Nextflow is a framework for the development of data workflows. It enables engineers and data scientists to create and securely deploy custom, parallel data applications in the cloud or on traditional, on-premise infrastructure. Nextflow is characterized by a powerful dataflow programming paradigm coupled with execution engines that allow for transparent deployment.</p> <p>Nextflow is both a programming workflow language and an execution runtime that supports a wide range of execution platforms including popular traditional grid scheduling systems such as Slurm and IBM LSF as well as cloud services such as AWS Batch and Google Cloud Life Sciences.</p>","title":"What is Nextflow?"},{"location":"#why-nextflow-tower","text":"<p>We created Nextflow in 2013 to deliver the most seamless experience for executing data workflows at scale. Tower is the continuation of that mission. Encompassing the latest technologies, we have built the solution to easily execute and monitor pipelines across every stage. Tower brings the cloud closer than ever before with automated resource provisioning and role-based access control.</p> <p>Tower has been designed from day one to be easily installable in any environment - data and compute never leaves your organization's security boundary. It has been extensively tested with over 125 million jobs to date and zero down time.</p> <p>As mandated by healthcare industries to ensure compliance, the Tower platform is regularly submitted to penetration tests and security scanning. These tests meet compliance standards set by ISO-27001, HIPAA, and HITRUST.</p>  <p>Tip</p> <p>Sign up to try Tower for free or request a demo for deployments in your own on-premise or cloud environment.</p>","title":"Why Nextflow Tower?"},{"location":"api/cli/","text":"<p>The Nextflow Tower API can be accessed through the Tower CLI. Please follow the link to learn more information on how to use the CLI for building your own automations for Tower.</p> <p></p>","title":"CLI"},{"location":"api/endpoints/","text":"<p>You can a detailed list of all Tower endpoints at this page. </p> <p>It also includes request and response payload examples, and the ability test each endpoint invocation interactively.  </p>","title":"Endpoints"},{"location":"api/openapi/","text":"<p>The most updated OpenAPI schema for Tower API is available at this link.</p>","title":"OpenAPI schema"},{"location":"api/overview/","text":"<p>Tower is API centric, it exposes a public API with all necessary calls to manage and monitor Nextflow workflows programmatically.  This allows organizations to extend their existing solutions by leveraging the Tower API.</p>","title":"Overview"},{"location":"api/overview/#schema","text":"<p>All API access is over HTTPS, and accessed from <code>https://api.tower.nf</code>. All data is sent and received as JSON encoded objects.</p> <p>All timestamps use the ISO 8601 date-time standard format:</p>  <p>Hint</p> <p>YYYY-MM-DDTHH:MM:SSZ</p>","title":"Schema"},{"location":"api/overview/#authentication","text":"<p>Tower API requires an authentication token to be specified in each API request using the  Bearer HTTP header.</p> <p>Your personal authorization token can be found in your settings, at the top-right corner of the page under the  Your tokens section.</p> <p></p> <p>To create a new access token, just provide a name for the token. This will help to identify it later.</p> <p></p> <p>Once created, the token can only be seen once, when it is initially created. It is important you keep this token at a safe place.</p> <p></p> <p>Once created, use the token to authenticate via cURL, Postman, or within your code against the Nextflow API to perform the necessary calls for completing your tasks.  Please remember that, as any other Bearer token, this token must be included in every API call.</p>","title":"Authentication"},{"location":"api/overview/#example-call-using-the-curl-command","text":"<pre>1</pre><pre><code>curl -H \"Authorization: Bearer eyJ...YTk0\" https://tower.nf/api/workflow\n</code></pre>   <p>Use your token in every API call</p> <p>Please remember that, as any other Bearer token, this token must be included in every API call. You can find at the following link more details about the Bearer token authentication. scheme.</p>","title":"Example call using the cURL command"},{"location":"api/overview/#parameters","text":"<p>Some API <code>GET</code> methods will accept standard <code>query</code> parameters, which are defined in the documentation; <code>querystring</code> optional  parameters such as page size, number (when available) and file name; and body parameters, mostly used for <code>POST</code>, <code>PUT</code> and <code>DELETE</code> requests.</p> <p>Additionally, several head parameters are accepted such as <code>Authorization</code> for bearer access token or <code>Accept-Version</code> to indicate the desired API version to use (default to version 1)</p> <pre>1\n2\n3\n4</pre><pre><code>curl -H \"Authorization: Bearer QH..E5M=\" \n     -H \"Accept-Version:1\"\n     -X POST https://tower.nf/api/domain/{item_id}?queryString={value}\n     -d { params: { \"key\":\"value\" } }\n</code></pre>","title":"Parameters"},{"location":"api/overview/#client-errors","text":"<p>There exists two typical standard errors, or non <code>200</code> or <code>204</code> status responses, to expect from the API.</p>","title":"Client errors"},{"location":"api/overview/#bad-request","text":"<p>The request payload is not properly defined or the query parameters are invalid.</p> <pre>1\n2\n3</pre><pre><code>{\n    \"message\": \"Oops... Unable to process request - Error ID: 54apnFENQxbvCr23JaIjLb\"\n}\n</code></pre>","title":"Bad request"},{"location":"api/overview/#forbidden","text":"<p>Your access token is invalid or expired. This response may also imply that the entry point you are trying to access is not available;  in such a case, it is recommended you check your request syntax.</p> <pre>1</pre><pre><code>Status: 403 Forbidden\n</code></pre>","title":"Forbidden"},{"location":"api/overview/#http-verbs","text":"<p>The following table describes the standard and API Rest verbs used.</p>    Verb Description     <code>HEAD</code> Can be issued against any resource to get just the HTTP header info   <code>GET</code> Used for retrieving resources   <code>POST</code> Used for creating resources   <code>PUT</code> Used for updating resources   <code>DELETE</code> Used for deleting resources","title":"HTTP verbs"},{"location":"api/overview/#rate-limiting","text":"<p>For all API requests, there is a threshold of 20 calls per second (72000 calls per hour) and access key. </p>","title":"Rate limiting"},{"location":"compute-envs/altair-pbs-pro/","text":"","title":"Altair PBS Pro"},{"location":"compute-envs/altair-pbs-pro/#overview","text":"<p>Altair PBS Pro is a workload manager and job scheduler tool provided by Altair Engineering, Inc.</p>","title":"Overview"},{"location":"compute-envs/altair-pbs-pro/#requirements","text":"<p>To launch pipelines into a Altair PBS Pro scheduler from Tower, the following requirements must be fulfilled:</p> <ul> <li>The cluster should be reachable via an SSH connection using an SSH key.</li> <li>The cluster should allow outbound connections to the Tower web service.</li> <li>The cluster queue used to run the Nextflow head job must be able to submit cluster jobs.</li> <li>The Nextflow runtime version 21.02.0-edge (or later) should be installed on the cluster.</li> </ul>","title":"Requirements"},{"location":"compute-envs/altair-pbs-pro/#compute-environment","text":"<p>Follow these steps to create a new compute environment for Altair PBS Pro:</p> <p>1. In a workspace choose \"Compute environments\" and then, click on the New Environment button.</p> <p>2. Enter a descriptive name (e.g. PBS Pro On-prem) and select Altair PBS Pro as the target platform.</p> <p>3. Select the + sign to add new SSH credentials.</p> <p>4. Enter a name for the credentials.</p> <p>5. Enter your SSH private key and associated Passphrase, if required then click Create.</p>  <p>Tip</p> <p>A passphrase for your SSH key may be optional depending on how it was created. See here for detailed instructions for how to create a key.</p>  <p>6. Enter the absolute path of the Work directory to be used on the cluster.</p> <p>7. Enter the absolute path of the Launch directory to be used on the cluster (optional).</p> <p>8. Enter the Login hostname. This is usually the cluster login node address.</p> <p>9. The Head queue name which is the name of the queue, on the cluster, used to launch the execution of the Nextflow runtime.</p> <p>10. The Compute queue name which is the name of the queue, on the cluster, to which pipeline jobs are submitted.</p>  <p>Tip</p> <p>The Compute queue can be overridden as a configuration option in the Nextflow pipeline configuration. See Nextflow docs for more details.</p>  <p>11. You can specify certain environment variables on the Head job or the Compute job using the Environment variables option.</p> <p></p> <p>Advanced options</p> <p>12. Optionally, you can customize Nextflow queue size field to control the number of Nextflow jobs submitted to the queue at the same time.</p> <p>13. Optionally, you can use the Head job submit options to  specify options to the head job.</p> <p>14. Select Create to finalize the creation of the compute environment.</p> <p>Jump to the documentation section for Launching Pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/aws-batch/","text":"","title":"AWS Batch"},{"location":"compute-envs/aws-batch/#overview","text":"<p>Disclaimer</p> <p>This guide assumes you have an existing Amazon Web Service (AWS) Account. Sign up for a free AWS account here.</p>  <p>There are two ways to create a Compute Environment for AWS Batch with Tower.</p> <ol> <li> <p>Tower Forge for AWS Batch: This option automatically manages the AWS Batch resources in your AWS account.</p> </li> <li> <p>Tower Launch: It allows you to create a compute environment using existing AWS Batch resources.</p> </li> </ol> <p>If you don't have an AWS Batch environment fully set-up yet, it is suggested to follow the Tower Forge guide. </p> <p>If your administrator has provided you with an AWS queue, or if you have set up AWS Batch previously, please follow the Tower Launch guide.</p>","title":"Overview"},{"location":"compute-envs/aws-batch/#forge","text":"<p>Warning</p> <p>Follow these instructions if you have not pre-configured an AWS Batch environment. Note that this will create resources in your AWS account that you may be charged for by AWS.</p>  <p>Tower Forge automates the configuration of an AWS Batch compute environment and queues required for the deployment of Nextflow pipelines.</p>","title":"Forge"},{"location":"compute-envs/aws-batch/#forge-aws-resources","text":"","title":"Forge AWS Resources"},{"location":"compute-envs/aws-batch/#iam-user-permissions","text":"<p>To use the Tower Forge feature, Tower requires an Identity and Access Management (IAM) user with the permissions listed in the following policy file. These authorizations are more permissive than those required to only launch a pipeline, since Tower needs to manage AWS resources on your behalf.</p> <p>The steps below will guide you through the creation of a new IAM user for Tower, plus how to attach the required policy for the newly created user.</p> <p>1. Open the AWS IAM console. 2 Select Users on the left menu and click the Add User button on top.</p> <p></p> <p>3. Enter a name for your user (e.g. <code>tower</code>) and choose the Programmatic access type. </p> <p>4 Select the Next: Permissions button.</p> <p>5. Click on the Next: Tags button, then Next: Review and Create User.</p> <p></p>  <p>This user has no permissions</p> <p>For the time being, you can ignore the warning. It will be addressed through our team using an IAM Policy later on.</p>  <p>6. Save the Access key ID and Secret access key in a secure location as we will be using these in the next section. </p> <p>7 Once you have saved the keys, press the Close button.</p> <p></p> <p>8. Back in the users table, select the newly created user and click + Add inline policy to add user permissions.</p> <p></p> <p>9. Copy the content of the policy linked above into the \"JSON\" tab as seen underneath.</p> <p></p> <p>10. Select the Review policy button, then name your policy (e.g. <code>tower-forge-policy</code>), and confirm the operation by clicking on the Create policy button.</p>  <p>What permissions are required?</p> <p>This policy includes the minimal permissions required to allow the user to submit jobs to AWS Batch, gather the container execution metadata, read CloudWatch logs and access data from the S3 bucket in your AWS account in read-only mode.</p>","title":"IAM User Permissions"},{"location":"compute-envs/aws-batch/#creating-an-s3-bucket-for-storage","text":"<p>S3 stands for Simple Storage Service and is a type of object storage. To access files and store the results for our pipelines, we have to create an S3 Bucket next.</p> <p>We must grant our new Tower IAM user access to this bucket.</p> <p>1. Navigate to S3 service </p> <p>2. Select Create New Bucket.</p> <p>3. Enter a unique name for your Bucket and select a region.</p> <p></p>  <p>Which AWS region should I use?</p> <p>The region of the bucket should be in the same region as the compute environment which we will set in the next section. Typically users select a region closest to their physical location but Tower Forge supports creating resources in any of the available AWS regions.</p>  <p>4. Select the default options for Configure options.</p> <p></p> <p>5. Select the default options for Set permissions.</p> <p></p> <p>6. Review the bucket and select Create bucket.</p> <p></p>  <p>S3 Storage Costs</p> <p>S3 is used by Nextflow for the storage of intermediary files. For production pipelines, this can amount to a large quantity of data. To reduce costs, when configuring a bucket, users should consider management options such as the ability to automatically delete these files after 30 days. For more information on this process, click [here] (https://aws.amazon.com/premiumsupport/knowledge-center/s3-empty-bucket-lifecycle-rule/).</p>","title":"Creating an S3 Bucket for Storage"},{"location":"compute-envs/aws-batch/#forge-compute-environment","text":"<p>Awesome!</p> <p>You have completed the AWS environment setup for Tower.</p>  <p>Now we can add a new AWS Batch environment in the Tower User Interface (UI). To create a new compute environment, follow these steps:</p> <p>1. In a workspace choose \"Compute environments\" and then, click on the New Environment button.</p> <p>2. Enter a descriptive name for this environment. </p> <p>For example, AWS Batch Spot (eu-west-1) and select Amazon Batch as the target platform.</p> <p></p> <p>3. Add new credentials by selecting the + button. </p> <p>4. Choose a name, e.g. AWS Credentials.</p> <p>5. Add the Access key and Secret key. </p> <p>These are the keys we saved in the previous steps when creating the AWS IAM user.</p> <p></p>  <p>Multiple credentials</p> <p>You can create multiple credentials in your Tower environment.</p>  <p>6. Select a Region, for example eu-west-1 - Europe (Ireland), and enter the S3 bucket we created in the previous section in the Pipeline work directory e.g: <code>s3://unique-tower-bucket</code>.</p> <p>7. Select Batch Forge as the Config Mode.</p>  <p>Warning</p> <p>The bucket should be in the same Region as selected above.</p>  <p></p> <p>8. Choose a Provisioning model. </p> <p>In most cases this will be Spot.</p>  <p>Spot or On-demand?</p> <p>You can choose to create a compute environment that will launch either Spot or On-demand instances. Spot instances can cost as little as 20% of on-demand instances and with Nextflow's ability to automatically relaunch failed tasks, Spot is almost always the recommended provisioning model. Note, however, that when choosing Spot instances, Tower will also create a dedicated queue for running the main Nextflow job using a single on-demand instance in order to prevent any execution interruptions.</p>  <p></p> <p>9. Enter the Max CPUs e.g. <code>64</code>. </p> <p>This is the maximum number of combined CPUs (the sum of all instances CPUs) AWS Batch will launch at any time.</p> <p>10. Choose EBS Auto scale to allow the EC2 virtual machines to expand the amount of available disk space during task execution.</p> <p>11. With the optional Enable Fusion mounts feature enabled, S3 buckets specified in the Pipeline work directory and Allowed S3 Buckets fields will be mounted as file system volumes in the EC2 instances carrying out the Batch job execution. These are then accessible at the path location with the pattern <code>/fusion/s3/BUCKET_NAME</code>.</p> <p>For example if the bucket name is <code>s3://imputation-gp2</code>, the Nextflow pipeline will access it using the file system path <code>/fusion/s3/imputation-gp2</code>.</p>  <p>Tip</p> <p>Note that it is not required to modify your pipeline or files to take advantage of this feature. Nextflow is able to recognise these buckets automatically and will replace any reference to s3:// prefixed files with the corresponding Fusion mount paths.</p>  <p>12. Choose Enable GPUs to allow the deployment of GPU enabled EC2 virtual machines if required.</p> <p>13. Enter any additional Allowed S3 buckets that your workflows require to read input data or to write output files. </p> <p>The Pipeline work directory bucket above is added by default to the list of Allowed S3 buckets.</p> <p>14. To use EFS, you can either specify the existing EFS option using Use existing EFS file system or deploy a new EFS using Create new EFS file system option.</p> <p>15. To use FSx, you can enter <code>/fsx</code> as the FSx mount path and set the Pipeline work directory above to be <code>/fsx/work</code></p> <p></p> <p>16. Choose the Dispose resources option.</p> <p>17. You can specify certain environment variables on the Head job or the Compute job using the Environment variables option.</p> <p></p> <p>Advanced options</p> <p>18. Optionally, you can specify the Allocation strategy and indicate the preferred Instance types to AWS Batch.</p> <p>19. You can configure your custom networking setup using the <code>VPC</code>, <code>Subnets</code> and <code>Security groups</code> fields. </p>  <p>AMI ID - AMI requirements for AWS Batch use</p> <p>To use an existing AMI, make sure the AMI is based on an Amazon Linux-2 ECS optimized image that meets the Batch requirements. To learn more about approved versions of the Amazon ECS optimized AMI, visit this link</p>   <p>Remote access</p> <p>If you need to debug the EC2 instance provisioned by AWS Batch, specify the <code>Key pair</code> to login the VM via SSH.</p>  <p>20. Optionally, you can set Min CPUs to be greater than <code>0</code>, EC2 instances will remain active. An advantage of this is that a pipeline execution will initialize faster.</p>  <p>Min CPUs - Editing this will result in additional AWS costs</p> <p>Keeping EC2 instances running may result in additional costs. You will be billed for these running EC2 instances regardless of whether you are executing pipelines or not.</p>  <p></p> <p>21. You can specify the hardware resources allocated for the Head Job using Head Job CPUs and Head Job Memory</p> <p>22. For fine-grained IAM permissions for the Head Job and Compute Job, you can rely upon Head Job role and Compute Job role </p> <p>23. If you're using the <code>Spot instances</code>, then you could also specify the <code>Cost percentage</code> to determine the maximum percentage that a <code>Spot instance</code> price can be, when compared with the <code>On-Demand</code> price for that instance type, before instances are launched.</p> <p> </p> <p>23. Optionally, you can also specify the location of <code>aws</code> cli using the AWS CLI tool path.</p> <p>24. Select Create to finalize the compute environment setup. </p> <p>It will take a few seconds for all the resources to be created and then you will be ready to launch pipelines.</p> <p> </p> <p>Jump to the documentation section for Launching Pipelines.</p>","title":"Forge compute environment"},{"location":"compute-envs/aws-batch/#manual-enabling","text":"<p>This section is for users with a pre-configured AWS environment. You will need a Batch queue, Batch compute environment, an IAM user and an S3 bucket already set up.</p> <p>To enable Tower within your existing AWS configuration, you need to have an IAM user with the following IAM permissions:</p> <ul> <li><code>AmazonS3ReadOnlyAccess</code></li> <li><code>AmazonEC2ContainerRegistryReadOnly</code></li> <li><code>CloudWatchLogsReadOnlyAccess</code></li> <li>A custom policy to grant the ability to submit and control Batch jobs.</li> <li>Write access to any S3 bucket used pipeline work directories with the following policy template. See below for details</li> </ul> <p>With these permissions set, we can add a new AWS Batch environment in the Tower UI</p>","title":"Manual Enabling"},{"location":"compute-envs/aws-batch/#manual-compute-environment","text":"<p>To create a new compute environment for AWS Batch (Manual):</p> <p>1. In a workspace choose \"Compute environments\" and then, click on the New Environment button.</p> <p>2. Choose a descriptive name for this environment. </p> <p>For example \"AWS Batch Launch (eu-west-1)\".</p> <p>3. Select Amazon Batch as the target platform.</p> <p> </p> <p>4. Add new credentials by clicking the \"+\" button. </p> <p>5. Choose a name, add the Access key and Secret key from your IAM user.</p> <p> </p>  <p>Multiple credentials</p> <p>You can create multiple credentials in your Tower environment. See the section Credentials Management.</p>  <p>6. Select a region, for example \"eu-west-1 - Europe (Ireland)\"</p> <p>7. Enter an S3 bucket path, for example \"s3://tower-bucket\"</p> <p>8. the Manual config mode.</p> <p>9. Enter the Head queue, which is the name of the AWS Batch queue that the Nextflow driver job will run. </p> <p>10. Enter the Compute queue, which is the name of the AWS Batch queue that tasks will be submitted to.</p> <p>11. Select Create to finalize the compute environment setup.</p> <p> </p> <p>12. You can specify certain environment variables on the Head job or the Compute job using the Environment variables option.</p> <p></p> <p>Advanced options</p> <p>13. You can specify the hardware resources allocated for the Head Job using Head Job CPUs and Head Job Memory</p> <p>14. For fine-grained IAM permissions for the Head Job and Compute Job, you can rely upon Head Job role and Compute Job role </p> <p>15. Optionally, you can also specify the location of <code>aws</code> cli using the AWS CLI tool path.</p> <p>16. Select Create to finalize the compute environment setup. </p>","title":"Manual compute environment"},{"location":"compute-envs/aws-batch/#access-to-s3-buckets","text":"<p>Tower can use S3 to access data, create work directories and write outputs. The IAM user above needs permissions to use these S3 Buckets. We can set a policy for our IAM user that provides the permission to access specific buckets.</p> <p>1. Access the IAM User table in the IAM service</p> <p>2. Select the IAM user.</p> <p>3. Select + add inline policy.</p> <p> </p> <p>4. Select JSON and copy the contents of this policy. Replace lines 10 and 21 with your bucket name.</p> <p> </p> <p>5. Name your policy and click Create policy.</p> <p>Jump to the documentation section for Launching Pipelines.</p>","title":"Access to S3 Buckets"},{"location":"compute-envs/azure-batch/","text":"","title":"Azure Batch"},{"location":"compute-envs/azure-batch/#overview","text":"<p>Warning</p> <p>The Tower support for Azure Batch is currently in beta. Any feedback and suggestions are welcome.    </p>   <p>Disclaimer</p> <p>This guide assumes you have an existing Azure Account. Sign up for a free Azure account here.</p>  <p>There are two ways to create a Compute Environment for Azure Batch with Tower.</p> <ol> <li> <p>Tower Forge: This option allows for Azure Batch to automatically create Azure Batch resources in your Azure account.</p> </li> <li> <p>Tower Launch: This allows you to create a compute environment using existing Azure Batch resources.</p> </li> </ol> <p>If you don't yet have an Azure Batch environment fully set up, it is suggested that you follow the Tower Forge guide. </p> <p>If you have been provided with an Azure Batch queue from your account administrator or if you have set up Azure Batch previously, directly follow the Tower Launch guide.</p>","title":"Overview"},{"location":"compute-envs/azure-batch/#forge","text":"<p>Warning</p> <p>Follow these instructions if you have not pre-configured an Azure Batch environment. Note that this will create resources in your Azure account that you may be charged for by Azure.</p>","title":"Forge"},{"location":"compute-envs/azure-batch/#resource-group","text":"<p>To create the necessary Azure Batch and Azure Storage accounts, we must first create a Resource Group in the region of your choice.</p> <p>When you open this link, you'll notice the Create new resource group dialogue as shown below.</p> <p> </p> <p>1. Add the name for the resource group (e.g. <code>towerrg</code>). </p> <p>2. Select the preferred region for this resource group. </p> <p>3. Click Review and Create to proceed to the review screen.</p> <p>4. Click Create to create the resources.</p>","title":"Resource group"},{"location":"compute-envs/azure-batch/#storage-account","text":"<p>The next step is to create the necessary Azure Storage. </p> <p>When you open this link, you'll notice the Create a storage account dialogue as shown below.</p> <p> </p> <p>1. Add the name for the storage account (for e.g. <code>towerrgstorage</code>).</p> <p>2. Select the preferred region for this resource group.</p> <p>3. Click Review and Create to proceed to the review screen.</p> <p>4. Click Create to create the Azure Storage account.</p> <p>5. Next, create a Blob container within this storage account by navigating to your new storage account and clicking on Container as shown below.</p> <p> </p> <p>6. Create a new Blob container by clicking on the + Container option.</p> <p>A new container dialogue will open as shown below. Choose a suitable name (e.g. <code>towerrgstorage-container</code>).</p> <p> </p> <p>7. Once the new Blob container is created, navigate to the Access Keys section of the storage account (e.g. <code>towerrgstorage</code>).</p> <p>8. Store the access keys for the newly created Azure Storage account as is pictured underneath.</p> <p> </p>","title":"Storage account"},{"location":"compute-envs/azure-batch/#batch-account","text":"<p>The next step is to create the necessary Azure Storage. </p> <p>When you open this link, you'll notice the Create a batch account dialogue, as shown below.</p> <p> </p> <p>1. Add the name for the storage account (for e.g. <code>towerrgbatch</code>).</p> <p>2. Select the preferred region for this resource group.</p> <p>3. Click Review and Create to proceed to the review screen.</p> <p>4. Click Create to create the Azure Batch account.</p>  <p>Congratulations!</p> <p>You have completed the Azure environment setup for Tower.</p>","title":"Batch account"},{"location":"compute-envs/azure-batch/#forge-compute-environment","text":"<p>Tower Forge automates the configuration of an Azure Batch compute environment and queues required for the deployment of Nextflow pipelines.</p> <p>Once the Azure resource setup is done, we can add a new Azure Batch environment in the Tower UI. </p> <p>To create a new compute environment, follow these steps:</p> <p>1. In a workspace choose Compute environments and then, click on the New Environment button.</p> <p>2. Enter a descriptive name for this environment, for example, Azure Batch (east-us), and select Azure Batch as the target platform.</p> <p> </p> <p>3. Add new credentials by selecting the + button. </p> <p>4. Choose a name, e.g. tower credentials and add the Access key and Secret key. </p> <p>These are the keys we saved in the previous steps when creating the Azure resources.</p> <p> </p>  <p>Multiple credentials</p> <p>You can create multiple credentials in your Tower environment.</p>  <p>5. Select a Region, for example, eastus (East US), and in the Pipeline work directory enter the Azure blob container we created in the previous section e.g: <code>az://towerrgstorage-container/work</code>.</p> <p> </p>  <p>Warning</p> <p>The blob container should be in the same Region as selected above.</p>  <p>6. Select the Config mode as Batch Forge and, optionally, add the default VM type depending on your quota limits. </p> <p>The default VM type is <code>Standard_D4_v3</code>.</p> <p> </p> <p>7. Next, specify the maximum number of VMs you'd like to deploy in the <code>VMs count</code> field. </p> <p>8. Enable the Autoscale option, if you'd like to automatically scale up (<code>VMs count</code>) and down (<code>0</code> VMs) based on the number of tasks.</p> <p>9. Enable the Dispose resources option if you'd like Tower to automatically delete the deployed Pool once the workflow is complete.</p> <p>Advanced options</p> <p>10. Optionally, specify the Jobs cleanup policy to delete the jobs once the workflow's execution is completed.</p> <p> </p> <p>11. Optionally, specify the duration of the SAS token generated by Nextflow.</p> <p>12. Finally, click on Create to finalize the compute environment setup. It will take approximately 20 seconds for all the resources to be created and then you will be ready to launch pipelines.</p> <p> </p> <p>Jump to the documentation section for Launching Pipelines.</p>","title":"Forge compute environment"},{"location":"compute-envs/azure-batch/#launch","text":"<p>This section is for users with a pre-configured Azure environment. </p> <p>You will need an Azure Batch account, Azure Storage account already set up. </p> <p>To add a new compute environment in the Tower UI for existing Azure resources, follow these steps:</p> <p>1. In a workspace choose Compute environments and then, click on the New Environment button.</p> <p>2. Enter a descriptive name for this environment, for example, Azure Batch (east-us) and select Azure Batch as the target platform.</p> <p> </p> <p>3. Add new credentials by selecting the + button. </p> <p>4. Choose a name, e.g. tower credentials and add the Access key and Secret key. </p> <p>These are the keys we saved in the previous steps when creating the Azure resources.</p> <p> </p>  <p>Multiple credentials</p> <p>You can create multiple credentials in your Tower environment.</p>  <p>5. Select a Region, for example, eastus (East US), and in the Pipeline work directory enter the Azure blob container we created in the previous section e.g: <code>az://towerrgstorage-container/work</code>.</p> <p> </p>  <p>Warning</p> <p>The blob container should be in the same Region as selected above.</p>  <p>6. Select the Config mode as Manual. </p> <p>7. Add the name of the Azure Batch pool, provided to you by your Azure administrator, in the Compute Pool name section.</p> <p> </p> <p>8. You can specify certain environment variables on the Head job or the Compute job using the Environment variables option.</p> <p></p> <p>Advanced options</p> <p>9. Optionally, specify the Jobs cleanup policy to delete the jobs once the workflow's execution is completed.</p> <p> </p> <p>10. Optionally, specify the duration of the SAS token generated by Nextflow.</p> <p>11. Finally, click on Create to finalize the compute environment setup. </p> <p>It will take approximately 20 seconds for all the resources to be created and then you will be ready to launch pipelines.</p> <p> </p> <p>Jump to the documentation section for Launching Pipelines.</p>","title":"Launch"},{"location":"compute-envs/eks/","text":"","title":"Amazon EKS"},{"location":"compute-envs/eks/#overview","text":"<p>Amazon EKS is a managed Kubernetes cluster that allows the execution of containerized workloads in the AWS cloud at scale.</p> <p>Nextflow Tower offers native support for AWS EKS clusters and streamlines the deployment of Nextflow pipelines in such environments.</p>","title":"Overview"},{"location":"compute-envs/eks/#requirements","text":"<p>You need to have an EKS cluster up and running. Make sure you have followed the steps in the Cluster preparation guide to create the cluster resources required by Nextflow Tower.</p>","title":"Requirements"},{"location":"compute-envs/eks/#compute-environment-setup","text":"<p>1. In a workspace choose \"Compute environments\" and then, click on the New Environment button.</p> <p>2. Provide a name for this environment, for example, Amazon EKS (us-east-1).</p> <p>3. Select Amazon EKS as the target platform.</p> <p> </p> <p>4. Select your AWS credentials or create new ones. The credentials are needed to identify the user that will access the EKS cluster.</p>  <p>Tip</p> <p>Make sure the user has the IAM permissions required to describe and list EKS clusters as explained at this link.</p>  <p>5. Specify the AWS region where the Kubernetes cluster is located e.g. <code>us-west-1</code>.</p> <p>6. The field Cluster name lists all EKS clusters available in the selected region. Choose the one you want to use to deploy the Nextflow execution.</p> <p>7. Specify the Kubernetes Namespace that should be used to deploy the pipeline execution.</p> <p>If you have followed the example in the cluster preparation guide, this field should be <code>tower-nf</code>.</p> <p>8. Specify the Kubernetes Head service account that will be used to grant permissions to Tower to deploy the pod executions.</p> <p>If you have followed the cluster preparation guide, this field should be <code>tower-launcher-sa</code>.</p> <p>9. The Storage claim field allows you to specify the storage Nextflow will use as a scratch file system for the pipeline execution.</p> <p>This should reference a Kubernetes persistence volume with <code>ReadWriteMany</code> capability. See the cluster preparation guide for details.</p> <p>10. You can specify certain environment variables on the Head job or the Compute job using the Environment variables option.</p> <p></p>","title":"Compute environment setup"},{"location":"compute-envs/eks/#advanced-options","text":"<p>These options allow for the fine-tuning of the Tower configuration for the EKS cluster.</p> <p> </p> <p>The following parameters are available:</p> <p>1. The Storage mount path defines the file system path where the Storage claim is mounted. </p> <p>Default: <code>/scratch</code></p> <p>2. The Work directory field defines the file system path used as a working directory by Nextflow pipelines. It must be the same or a subdirectory of the Storage mount path at the previous point. </p> <p>Default: the same as Storage mount path.</p> <p>3. The  Compute service account field allows you to specify the Kubernetes service account that the pipeline jobs should use. </p> <p>Default is the <code>default</code> service account in your Kubernetes cluster.</p> <p>4. The pod behavior within the cluster could be controlled by using the Pod cleanup policy option.</p> <p>5. The Custom head pod specs field allows you to provide a custom configuration for the pod running the Nextflow workflow e.g. <code>nodeSelector</code> and <code>affinity</code> constraints. It should be a valid PodSpec YAML structure starting with <code>spec:</code>.</p> <p>6. The Custom service pod specs field allows you to provide a custom configuration for the compute environment service pod e.g. <code>nodeSelector</code> and <code>affinity</code> constraints. It should be a valid PodSpec YAML structure starting with <code>spec:</code>.</p> <p>Jump to the documentation section for Launching Pipelines.</p>","title":"Advanced options"},{"location":"compute-envs/gke/","text":"","title":"Google GKE"},{"location":"compute-envs/gke/#overview","text":"<p>Google GKE is a managed Kubernetes cluster that allows the execution of containerized workloads in Google Cloud at scale.</p> <p>Nextflow Tower offers native support for Google GKE clusters and streamlines the deployment of Nextflow pipelines in such environments.</p>","title":"Overview"},{"location":"compute-envs/gke/#requirements","text":"<p>You need to have a GKE cluster up and running. </p> <p>Make sure you have followed the steps in the Cluster preparation guide to create the cluster resources required by Nextflow Tower.</p>","title":"Requirements"},{"location":"compute-envs/gke/#compute-environment-setup","text":"<p>1. In a workspace choose \"Compute environments\" and then, click on the New Environment button.</p> <p>2. Enter the Name for this environment, for example, My GKE.</p> <p>3. Select Google GKE as the target platform.</p> <p></p> <p>4. Select your Google Cloud credentials. The credentials are needed to identify the user that will access the GKE cluster.</p> <p>5. Select the Location where the GKE cluster is located.</p>  <p>Regional and zonal clusters</p> <p>GKE clusters can be either regional or zonal. For example, <code>us-west1</code> identifies the United States West-Coast region, which has three zones: <code>us-west1-a</code>, <code>us-west1-b</code>, and <code>us-west1-c</code>.  Tower self-completion only shows regions. You should manually edit this field if your GKE cluster was created by zone rather than regionally. </p>  <p></p> <p>6. The field Cluster name lists all GKE clusters available in the selected location. Choose the one you want to use to deploy the Nextflow execution.</p> <p>7. Specify the Kubernetes Namespace that should be used to deploy pipeline executions.</p> <p>If you have followed the example in the cluster preparation guide, this field should be <code>tower-nf</code>.</p> <p>8. Specify the Kubernetes Head service account that will be used to grant permissions to Tower to deploy the pods executions and related.</p> <p>If you have followed the cluster preparation guide, this field should be <code>tower-launcher-sa</code>.</p> <p>9. The Storage claim field allows you to specify the storage Nextflow should use as a scratch file system for the pipeline execution.</p> <p>This should reference a Kubernetes persistence volume with <code>ReadWriteMany</code> capability. Check the cluster preparation guide for details.</p> <p>10. You can specify certain environment variables on the Head job or the Compute job using the Environment variables option.</p> <p></p>","title":"Compute environment setup"},{"location":"compute-envs/gke/#advanced-options","text":"<p>The following parameters are available:</p> <p>1. The Storage mount path defines the file system path where the Storage claim is mounted. </p> <p>Default: <code>/scratch</code></p> <p>2. The Work directory field defines the file system path used as a working directory by Nextflow pipelines. It must be the same or a subdirectory of the Storage mount path at the previous point. </p> <p>Default: the same as Storage mount path.</p> <p>3. The  Compute service account field allows you to specify the Kubernetes service account that the pipeline jobs should use. </p> <p>Default is the <code>default</code> service account in your Kubernetes cluster.</p> <p>4. The pod behavior within the cluster could be controlled by using the Pod cleanup policy option.</p> <p>5. The Custom head pod specs field allows you to provide a custom configuration for the pod running the Nextflow workflow e.g. <code>nodeSelector</code> and <code>affinity</code> constraints. It should be a valid PodSpec YAML structure starting with <code>spec:</code>.</p> <p>6. The Custom service pod specs field allows you to provide a custom configuration for the compute environment service pod e.g. <code>nodeSelector</code> and <code>affinity</code> constraints. It should be a valid PodSpec YAML structure starting with <code>spec:</code>.</p> <p>Jump to the documentation section for Launching Pipelines.</p>","title":"Advanced options"},{"location":"compute-envs/google-cloud/","text":"<p>Requirements</p> <p>This guide assumes you have an existing Google Cloud Account. Sign-up for a free account here.</p>","title":"Google Cloud Life Sciences"},{"location":"compute-envs/google-cloud/#cloud-life-sciences","text":"<p>Tower provides integration to Google Cloud via the Cloud Life Sciences API.</p> <p>The guide is split into two parts, namely how to configure your Google Cloud account and enable the Google Life Sciences Application Programming Interface (API), followed by a guide on how to create a new Google Cloud compute environment in Tower. </p>","title":"Cloud Life Sciences"},{"location":"compute-envs/google-cloud/#configuration-of-google-cloud","text":"","title":"Configuration of Google Cloud"},{"location":"compute-envs/google-cloud/#create-a-new-google-cloud-project-or-select-an-existing-one","text":"<p>Navigate to the Google Project Selector page and select an existing project or click CREATE PROJECT.</p> <p></p> <p>Enter a name for your new project e.g: \"tower-nf\". </p> <p>If you are part of an organization the location will be set by default to match your organization parameters.</p> <p></p>","title":"Create a new Google Cloud project or select an existing one."},{"location":"compute-envs/google-cloud/#make-sure-billing-is-enabled-for-the-project","text":"<p>At the top left of the page, in the navigation menu (\u2261) click Billing. You can follow the enable billing instructions here.</p> <p></p>","title":"Make sure Billing is enabled for the project."},{"location":"compute-envs/google-cloud/#enable-the-google-life-sciences-the-compute-engine-and-the-google-cloud-storage-apis","text":"<p>Open this link to enable all three APIs on for your project. </p> <p>Select your project from the drop down menu and click Enable. </p> <p>Alternatively enable these APIs manually by selecting the project on the top bar and visiting the API pages:</p> <p>1. Google Cloud Life Sciences API</p> <p>2. Compute Engine API</p> <p>3. Google Cloud Storage JSON API</p> <p></p>","title":"Enable the Google Life Sciences, the Compute Engine , and the Google Cloud Storage APIs."},{"location":"compute-envs/google-cloud/#retrieve-the-compute-engine-service-account-for-your-project","text":"<p>Click Go to credentials or visit this link</p> <p></p> <p>1. Select the Cloud Life Sciences API from the dropdown menu and select the radio button Yes, I'm using one or both to indicate we will use the Compute Engine API. </p> <p>2. Then click What credentials do I need?</p> <p></p> <p>3. A second screen appears to say you do not need any further credentials. Click Done.</p> <p></p> <p>You will be redirected to the API &amp; Services page and the Credentials section. </p>  <p>Disclaimer</p> <p>Note a Compute Engine default service account has been created. </p>  <p>4. Copy the email address as you will need this to configure Google Storage.</p>","title":"Retrieve the Compute Engine Service account for your project"},{"location":"compute-envs/google-cloud/#create-a-new-key-for-the-compute-service-account","text":"<p>1. Copy and click the Email of the service account.</p> <p></p> <p>2. Click \"Manage Service accounts. </p> <p>3. Select Add key and Create new key.</p> <p></p> <p>4. Select JSON as the key type. Then, press Create.</p> <p></p> <p>A JSON key will be downloaded to your computer. </p> <p>This is the credential that is being used by Tower. You you will need it to configure the Tower compute environment. In the Service accounts page, you can see your key is now active and you can manage it from there.</p> <p></p>","title":"Create a new key for the compute service account"},{"location":"compute-envs/google-cloud/#create-a-google-storage-bucket","text":"<p>In the top left of the page, there is a navigation menu (\u2261). Open it and then, proceed to click on Storage and Create Bucket.</p> <p></p> <p></p>","title":"Create a Google Storage bucket."},{"location":"compute-envs/google-cloud/#configure-your-bucket","text":"<p>Bucket Naming - No underscores _ !</p> <p>Do not use underscores in your bucket name. Use hyphens instead.</p>  <p>1. Name your bucket, you will need this name to configure the Tower environment.</p> <p>2. Select Region as the Location type and the Location for your bucket. You will need the Location to configure the Tower environment.</p> <p>3. Select Standard as the default storage class.</p> <p>4. Select Uniform as the Access control.</p> <p></p>  <p>Tip</p> <p>The Google Cloud Life Sciences API is available in limited number of locations, however, these locations are only used to store metadata about the pipeline operations. The location of the storage bucket and compute resources can be in any region.</p>","title":"Configure your Bucket"},{"location":"compute-envs/google-cloud/#set-bucket-permissions","text":"<p>1. In the Storage page, on the Browser section, click on the newly created storage.</p> <p></p> <p>2. Navigate to the Permissions tab.</p> <p>3. Click on + Add,</p> <p>4. Copy-paste the service account email created above into the <code>new members box</code> and add the following roles:</p> <p>Storage Admin</p> <p>Storage Legacy Bucket Owner</p> <p>Storage Legacy Object Owner</p> <p>Storage Object Creator </p> <p></p>  <p>Congratulations!</p> <p>You have created a project, enabled the necessary Google APIs, created a bucket and a JSON file containing required credentials. You are now ready to set up a new compute environment in Tower.</p>","title":"Set Bucket permissions"},{"location":"compute-envs/google-cloud/#tower-configuration","text":"<p>Warning</p> <p>The following guide to configure Tower assumes you have JSON keys for a configured Google Cloud account. You will also need the name and location of the Google storage bucket.</p>  <p>The sections above shows how to configure Google Cloud.</p> <p>To create a new compute environment for Google Cloud in Tower follow these steps:</p> <p>1. In a workspace choose \"Compute environments\" and then, click on the New Environment button.</p> <p>2. Enter a name for this environment, e.g. \"Google Cloud Life Sciences (europe-west2)\".</p> <p>3. Select Google Life Sciences as the target platform.</p> <p></p> <p>4. Select the + sign to add new credentials. </p> <p>5. Name your credentials. </p> <p>6. Copy &amp; paste the contents from the Google JSON key. </p> <p>If you do not have a JSON key follow this guide.</p> <p></p> <p>7. Select the Region and Zones where you'd like to deploy the workload. </p> <p>The Google Storage bucket created earlier should be accessible in the region.</p> <p></p> <p>8. You can leave the Location empty and Google will run the Life Sciences API Service in the closest available location.</p> <p>9. Enter the bucket URL in the Pipeline work directory e.g. gs://my-google-bucket-name.</p>  <p>Tip</p> <p>This is the name of your Google Storage bucket with the <code>gs://</code> prefix.</p>  <p>10. You can also opt-in to execute the workflow on the Preemptible instances to save further cost.</p> <p>11. If you'd like to integrate an existing Google FileStore volume to your compute environment, you can make use of the Filestore file system option.</p> <p></p> <p>12. You can specify certain environment variables on the Head job or the Compute job using the Environment variables option.</p> <p></p>","title":"Tower configuration"},{"location":"compute-envs/google-cloud/#advanced-options","text":"<p>1. Optionally, settings such as Use Private Address, Boot disk size, Head Job CPUs and Head Job memory could be optimized as per the requirements of the workflow as well.</p> <p>2. Select Create to finalise the creation of the compute environment.</p> <p></p> <p>Jump to the documentation section for Launching Pipelines.</p>","title":"Advanced options"},{"location":"compute-envs/grid-engine/","text":"","title":"Grid Engine"},{"location":"compute-envs/grid-engine/#overview","text":"<p>Grid engine is a workload management tool maintained by Altair.</p>","title":"Overview"},{"location":"compute-envs/grid-engine/#requirements","text":"<p>To launch pipelines into a Grid engine scheduler from Tower, the following requirements must be fulfilled:</p> <ul> <li>The cluster should be reachable via an SSH connection using an SSH key.</li> <li>The cluster should allow outbound connections to the Tower web service.</li> <li>The cluster queue used to run the Nextflow head job must be able to submit cluster jobs.</li> <li>The Nextflow runtime version 21.02.0-edge (or later) should be installed on the cluster.</li> </ul>","title":"Requirements"},{"location":"compute-envs/grid-engine/#compute-environment","text":"<p>Follow these steps to create a new compute environment for Grid Engine:</p> <p>1. In a workspace choose \"Compute environments\" and then, click on the New Environment button.</p> <p>2. Enter a descriptive name (e.g. Grid Engine On-prem).</p> <p>3. Select Grid Engine as the target platform.</p> <p>4. Select the + sign to add new SSH credentials.</p> <p>5. Enter a name for the credentials.</p> <p>6. Enter your SSH private key and associated Passphrase, if required then click Create.</p>  <p>Tip</p> <p>A passphrase for your SSH key may be optional depending on how it was created. See here for detailed instructions for how to create a key.</p>  <p>7. Enter the absolute path of the Work directory to be used on the cluster.</p> <p>8. Enter the absolute path of the Launch directory to be used on the cluster.</p> <p>9. Enter the Login hostname. This is usually the cluster login node address.</p> <p>10. The Head queue name which is the name of the queue, on the cluster, used to launch the execution of the Nextflow runtime.</p> <p>11. The Compute queue name which is the name of the queue, on the cluster, to which pipeline jobs are submitted.</p>  <p>Tip</p> <p>The Compute queue can be overridden as a configuration option in the Nextflow pipeline configuration. See Nextflow docs for more details.</p>  <p>12. You can specify certain environment variables on the Head job or the Compute job using the Environment variables option.</p> <p></p> <p>Advanced options</p> <p>12. Optionally, you can customize Nextflow queue size field to control the number of Nextflow jobs submitted to the queue at the same time.</p> <p>13. Optionally, you can use the Head job submit options to  specify options to the head job.</p> <p>14. Select Create to finalize the creation of the compute environment.</p>  <p>Congratulations!</p> <p>You are now ready to launch pipelines.</p>  <p>Jump to the documentation section for Launching Pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/k8s/","text":"","title":"Kubernetes"},{"location":"compute-envs/k8s/#overview","text":"<p>Kubernetes is the leading technology for deployment and orchestration of containerized workloads in cloud-native environments.</p> <p>Tower streamlines the deployment of Nextflow pipelines into Kubernetes both in the cloud and in on-premises solutions.</p>","title":"Overview"},{"location":"compute-envs/k8s/#requirements","text":"<p>You need to have Kubernetes cluster up and running.  Make sure you have followed the steps in the Cluster preparation guide to create the cluster resources required by Nextflow Tower.</p> <p>The following instructions are for a generic Kubernetes distribution. </p> <p>If you are using Amazon EKS or Google GKE, see the corresponding documentation pages.</p>","title":"Requirements"},{"location":"compute-envs/k8s/#compute-environment-setup","text":"<p>1. In a workspace choose \"Compute environments\" and then, click on the New Environment button.</p> <p>2. Enter a name to identify it (e.g. My K8s cluster).</p> <p>3. Select Kubernetes as the target platform.</p> <p></p> <p>4. Select an existing Kubernetes credentials or click the + button to create a new one.</p> <p>5. Give a name to this new credentials record.</p> <p>6. Enter the Kubernetes Service account token and then click Create.</p> <p></p>  <p>Tip</p> <p>The token can be found using the following command:</p> <pre>1\n2</pre><pre><code>SECRET=$(kubectl get secrets | grep &lt;SERVICE-ACCOUNT-NAME&gt; | cut -f1 -d ' ')\nkubectl describe secret $SECRET | grep -E '^token' | cut -f2 -d':' | tr -d '\\t'\n</code></pre>  <p>Replace <code>&lt;SERVICE-ACCOUNT-NAME&gt;</code> with the name of the service account create in the Cluster preparation step. If you followed the example in the guide, it should be <code>tower-launcher-sa</code>.</p>  <p>7. Enter Kubernetes Master server URL</p>  <p>Tip</p> <p>The master server can be found using the following command: <code>kubectl cluster-info</code></p>  <p>8. Enter the SSL Certificate to authenticate your connection.</p>  <p>Tip</p> <p>The certificate data can be found in your <code>~/.kube/config</code> file, check for the <code>certificate-authority-data</code> field matching to the specified server URL.</p>  <p>9. Specify Kubernetes Namespace that should be used to deployment the pipeline execution.</p> <p>If you have followed the example in the cluster preparation guide this field should be <code>tower-nf</code>.</p> <p>10. Specify the Kubernetes Head service account that will be used to grant permissions to Tower to deploy the pods executions and related.</p> <p>If you have followed the cluster preparation guide this field should be <code>tower-launcher-sa</code>.</p> <p>11. The Storage claim field allows you to specify the storage that Nextflow should use as a scratch file system for the pipeline execution.</p> <p>This should reference a Kubernetes persistence volume with <code>ReadWriteMany</code> capability.</p> <p>Check the cluster preparation guide for details.</p> <p>12. You can specify certain environment variables on the Head job or the Compute job using the Environment variables option.</p> <p></p>","title":"Compute environment setup"},{"location":"compute-envs/k8s/#advanced-options","text":"<p>These options allow the fine-tuning of the Tower configuration for the Kubernetes cluster.</p> <p></p> <p>The following parameters are available:</p> <p>1. The Storage mount path defines the file system path where the Storage claim is mounted. </p> <p>Default: <code>/scratch</code></p> <p>2. The Work directory field defines the file system path used as a working directory by Nextflow pipelines. It must be the same or a subdirectory of the Storage mount path at the previous point. </p> <p>Default: the same as Storage mount path.</p> <p>3. The  Compute service account field allows you to specify the Kubernetes service account that the pipeline jobs should use. </p> <p>Default is the <code>default</code> service account in your Kubernetes cluster.</p> <p>4. The pod behavior within the cluster could be controlled by using the Pod cleanup policy option.</p> <p>5. The Custom head pod specs field allows you to provide a custom configuration for the pod running the Nextflow workflow e.g. <code>nodeSelector</code> and <code>affinity</code> constraints. It should be a valid PodSpec YAML structure starting with <code>spec:</code>.</p> <p>6. The Custom service pod specs field allows you to provide a custom configuration for the compute environment service pod e.g. <code>nodeSelector</code> and <code>affinity</code> constraints. It should be a valid PodSpec YAML structure starting with <code>spec:</code>.</p> <p>Jump to the documentation section for Launching Pipelines.</p>","title":"Advanced options"},{"location":"compute-envs/lsf/","text":"","title":"IBM LSF"},{"location":"compute-envs/lsf/#overview","text":"<p>IBM Spectrum LSF is an IBM workload management solution that for HPC. LSF aims to enhance user and administrator experience, reliability and performance at scale.</p>  <p>Note</p> <p>This feature enables Tower to connect to remote cloud or on-premise clusters and launch pipelines.</p>","title":"Overview"},{"location":"compute-envs/lsf/#requirements","text":"<p>To launch pipelines into a LSF managed cluster from Tower, the following requirements must to be fulfilled:</p> <ul> <li>The cluster should be reachable via an SSH connection using an SSH key.</li> <li>The cluster should allow outbound connections to the Tower web service.</li> <li>The cluster queue used to run the Nextflow head job must be able to submit cluster jobs.</li> <li>The Nextflow runtime version 21.02.0-edge (or later) should be installed on the cluster.</li> </ul>","title":"Requirements"},{"location":"compute-envs/lsf/#compute-environment","text":"<p>Follow these steps to create a new compute environment for LSF:</p> <p>1. In a workspace choose \"Compute environments\" and then, click on the New Environment button.</p> <p>2. Enter a descriptive name (e.g. LSF On-premise).</p> <p>3. Select IBM LSF as the target platform.</p> <p>4. Select the + sign to add new SSH credentials.</p> <p>5. Enter a name for the credentials.</p> <p>6. Enter your SSH private key and associated Passphrase, if required. Then, select Create.</p>  <p>Tip</p> <p>A passphrase for your SSH key may be optional depending on how it was created. See here for detailed instructions for how to create a key.</p>  <p>7. Enter the absolute path of the Work directory to be used on the cluster.</p> <p>8. Enter the absolute path of the Launch directory to be used on the cluster.</p> <p>9. Enter the Login hostname. This is usually the cluster login node address.</p> <p>10. The Head queue name which is the name of the queue, on the cluster, used to launch the execution of the Nextflow runtime.</p> <p>11. The Compute queue name which is the name of the queue, on the cluster, to which pipeline jobs are submitted.</p>  <p>Tip</p> <p>The Compute queue can be overridden as a configuration option in the Nextflow pipeline configuration. See Nextflow docs for more details.</p>  <p>12. You can specify certain environment variables on the Head job or the Compute job using the Environment variables option.</p> <p></p> <p>Advanced options</p> <p>1. Optionally, you can customize Nextflow queue size field to control the number of Nextflow jobs submitted to the queue at the same time.</p> <p>2. Optionally, you can also use the Unit for memory limits section to customize the memory limits of your LSF cluster.</p> <p>3. Optionally, you can use the Head job submit options to specify options to the head job.</p> <p>4. Select Create to finalize the creation of the compute environment.</p> <p>Jump to the documentation section for Launching Pipelines.</p>","title":"Compute environment"},{"location":"compute-envs/overview/","text":"","title":"Overview"},{"location":"compute-envs/overview/#introduction","text":"<p>Tower uses the concept of Compute Environments to define the execution platform where a pipeline will run. </p> <p>It supports launching of pipelines into a growing number of cloud and on-premise infrastructures.</p> <p></p> <p>Each compute environment must be pre-configured to enable Tower to submit tasks. You can read more on how to set up each environment using the links below.</p>","title":"Introduction"},{"location":"compute-envs/overview/#setup-guides","text":"<p>The following sections describe how to set up each of the available compute environments.</p> <ul> <li>AWS Batch</li> <li>Azure Batch</li> <li>Google Cloud</li> <li>IBM LSF</li> <li>Slurm</li> <li>Grid Engine</li> <li>Altair PBS Pro</li> <li>Amazon Kubernetes (EKS)</li> <li>Google Kubernetes (GKE)</li> <li>Hosted Kubernetes</li> </ul>","title":"Setup guides"},{"location":"compute-envs/overview/#select-a-default-compute-environment","text":"<p>If you have more than one Compute Environment, you can select which one will be used by default when launching a pipeline.</p> <p>1. Navigate to your compute environments.</p> <p>2. Choose your default environment by selecting the Make primary button.   </p>  <p>Congratulations!</p> <p>You are now ready to launch pipelines with your primary compute environment.</p>","title":"Select a default compute environment"},{"location":"compute-envs/slurm/","text":"","title":"Slurm"},{"location":"compute-envs/slurm/#overview","text":"<p>Slurm Workload Manager is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters.</p>  <p>Note</p> <p>This feature enables Tower to connect to remote cloud or on-premise clusters and launch pipelines.</p>","title":"Overview"},{"location":"compute-envs/slurm/#requirements","text":"<p>To launch pipelines into a Slurm cluster from Tower, the following requirements must be fulfilled:</p> <ul> <li>The cluster should be reachable via an SSH connection using an SSH key.</li> <li>The cluster should allow outbound connections to the Tower web service.</li> <li>The cluster queue used to run the Nextflow head job must be able to submit cluster jobs.</li> <li>The Nextflow runtime version 21.02.0-edge (or later) should be installed on the cluster.</li> </ul>","title":"Requirements"},{"location":"compute-envs/slurm/#compute-environment","text":"<p>Follow these steps to create a new compute environment for Slurm:</p> <p>1. In a workspace choose \"Compute environments\" and then, click on the New Environment button.</p> <p>2. Enter a descriptive name (e.g. Slurm On-premise) and select Slurm Workload Manager as the target platform.</p> <p>3. Select the + sign to add new SSH credentials.</p> <p>4. Enter a name for the credentials.</p> <p>5. Enter your SSH private key and associated Passphrase, if required then click Create.</p> <p></p>  <p>Tip</p> <p>A passphrase for your SSH key may be optional depending on how it was created. See here for detailed instructions for how to create a key.</p>  <p>6. Enter the absolute path of the Work directory to be used on the cluster.</p> <p>7. Enter the absolute path of the Launch directory to be used on the cluster.</p> <p>8. Enter the Login hostname. This is usually the cluster login node address.</p> <p>9. The Head queue name which is the name of the queue, on the cluster, used to launch the execution of the Nextflow runtime.</p> <p>10. The Compute queue name which is the name of queue, on the cluster, to which pipeline jobs are submitted.</p>  <p>Tip</p> <p>The Compute queue can be overridden as a configuration option in the Nextflow pipeline configuration. See Nextflow docs for more details.</p>  <p>11. You can specify certain environment variables on the Head job or the Compute job using the Environment variables option.</p> <p></p> <p>Advanced options</p> <p>1. Optionally, you can customize Nextflow queue size field to control the number of Nextflow jobs submitted to the queue at the same time.</p> <p>2. Optionally, you can use the Head job submit options to  specify options to the head job.</p> <p>3. Select Create to finalize the creation of the compute environment.</p> <p>Jump to the documentation section for Launching Pipelines.</p>","title":"Compute environment"},{"location":"credentials/overview/","text":"","title":"Overview"},{"location":"credentials/overview/#introduction","text":"<p>Tower uses the concept of Credentials to store the keys and tokens necessary to access the Compute Environments as well as Git hosting services. The instructions for adding the credentials are mentioned in the documentation for respective compute environment or git hosting section of this documentation. </p> <p></p>  <p>Note</p> <p>All credentials are securely stored using advanced encryption (AES-256) and never exposed by any Tower API.</p>","title":"Introduction"},{"location":"datasets/overview/","text":"","title":"Datasets"},{"location":"datasets/overview/#overview","text":"<p>The Datasets functionality in Nextflow Tower allows the users to store CSV and TSV formatted files within a workspace and use those as an input one or more pipelines.</p> <p></p>  <p>Note</p> <p>This feature is available only in the organization workspaces..</p>","title":"Overview"},{"location":"datasets/overview/#creating-a-new-dataset","text":"<p>To create a new dataset, please follow these steps </p> <ol> <li> <p>Open the <code>Datasets</code> tab in your organization workspace.</p> </li> <li> <p>Click on <code>New dataset</code> button to open the dataset creation dialog as shown below.</p> </li> </ol> <p></p> <ol> <li> <p>You can enter the Name and Description fields as per the nature of your dataset.</p> </li> <li> <p>You can add the dataset file to your workspace using either drag and drop or by using the system file explorer dialog.</p> </li> <li> <p>It is possible to customize the subsequent views for the dataset using <code>First row as header</code> option, to accomodate the situations where the first row contains the column names.</p> </li> </ol>  <p>Warning</p> <p>The size of the dataset file cannot exceed 10MB.</p>","title":"Creating a new Dataset"},{"location":"datasets/overview/#dataset-versions","text":"<p>The Datasets functionality can accommodate multiple versions of a dataset. To add a new version for a dataset, please follow these steps </p> <ol> <li> <p>Click on the Edit option for the intended dataset.</p> </li> <li> <p>In the Edit dialog, click on the Add a new version button.</p> </li> <li> <p>Upload the newer version of the dataset and click on Update.</p> </li> </ol>  <p>Warning</p> <p>All subsequent versions of a dataset must be in the same data format as the initial version of the dataset.</p>","title":"Dataset versions"},{"location":"datasets/overview/#using-a-dataset","text":"<p>To use a dataset with the saved pipelines in your workspace, please follow these steps </p> <ol> <li> <p>Open any pipeline from the Launchpad containing a pipeline-schema.</p> </li> <li> <p>Click on the input field for the pipeline, removing any default value. </p> </li> <li> <p>Pick the right dataset for your pipeline</p> </li> </ol> <p></p>  <p>Warning</p> <p>The Datasets shown in the dropdown menu depends upon the validation specified in your pipeline-schema. Hence, if the schema specifies only <code>CSV</code> format, no <code>TSV</code> dataset would appear in the dropdown.</p>","title":"Using a Dataset"},{"location":"getting-started/deployments/","text":"<p>Tower can be accessed and/or deployed in three ways:</p> <ul> <li> <p>Hosted: The hosted version of Tower is available free of charge at tower.nf. This version is for individuals and organizations that want to get setup fast. It is the recommended way for users to become familiar with Tower. The service is hosted by Seqera Labs.</p> </li> <li> <p>Community deployment: Installation of the Tower community edition can be performed on a user's own system. The community edition has basic features for the monitoring of pipelines by an individual user.</p> </li> <li> <p>Enterprise deployment: Deployment of the fully-featured Tower application in an organization's own cloud or on-premise environment. This deployment option is supported by Seqera Labs and is recommended for production environments.</p> </li> </ul>","title":"Deployments"},{"location":"getting-started/deployments/#hosted","text":"<p>To try Tower, visit tower.nf and login with GitHub or Google credentials. The Launching Pipelines documentation section provides step-by-step instructions to start your first pipeline. The Hosted version of Tower has a limit of five concurrent workflow executions per user.</p> <p></p>","title":"Hosted"},{"location":"getting-started/deployments/#community","text":"<p>For more information on installing the Community version of Tower visit our GitHub repository and follow our deployment guide.</p> <p></p>","title":"Community"},{"location":"getting-started/deployments/#enterprise","text":"<p>Tower Enterprise is installed within an organization's own cloud or on-premise environment. It includes:</p> <ul> <li>Monitoring, logging &amp; observability</li> <li>Pipeline execution launchpad</li> <li>Cloud resource provisioning</li> <li>Pipeline actions and event-based execution</li> <li>LDAP &amp; OpenID Authentication</li> <li>Enterprise role-based access control</li> <li>Fully featured API</li> <li>Support for Nextflow &amp; Tower</li> </ul> <p>To install the Tower in your organization, contact Seqera Labs for a demo and to discuss your requirements.</p> <p></p>","title":"Enterprise"},{"location":"getting-started/usage/","text":"<p>You can use Tower via either the online GUI, using the <code>-with-tower</code> option with the Nextflow run command, or through the API.</p>","title":"Usage"},{"location":"getting-started/usage/#via-online-gui","text":"<p>1. Create an account and login into Tower, available free of charge, at tower.nf.</p> <p>2. Create and configure a new compute environment.</p> <p>3. Start launching pipelines.</p>","title":"Via online GUI"},{"location":"getting-started/usage/#via-nextflow-run-command","text":"<p>Create an account and login into Tower.</p> <p>1. Create a new token</p> <p>You can access your tokens from the Settings drop-down menu:</p> <p></p> <p>2. Name your token</p> <p></p> <p>3. Save your token safely</p> <p>Copy and keep your new token in a safe place.</p> <p></p> <p>4. Export your token</p> <p>Once your token has been created, open a terminal and type:</p> <pre>1\n2</pre><pre><code>export TOWER_ACCESS_TOKEN=eyxxxxxxxxxxxxxxxQ1ZTE=\nexport NXF_VER=20.10.0\n</code></pre>  <p>Where <code>eyxxxxxxxxxxxxxxxQ1ZTE=</code> is the token you have just created.</p>  <p>check your Nextflow version</p> <p>Bearer token requires Nextflow version 20.10.0 or later, set with the second command above.</p>  <p>To submit a pipeline to a Workspace using the Nextflow command line tool, add the workspace ID to your environment. For example</p> <pre>1</pre><pre><code>export TOWER_WORKSPACE_ID=000000000000000\n</code></pre>  <p>The workspace ID can be found on the organisation's Workspaces overview page.</p> <p>5. Run Nextflow with tower</p> <p>Run your Nextflow workflows as usual with the addition of the <code>-with-tower</code> command:</p> <pre>1</pre><pre><code>nextflow run hello.nf -with-tower\n</code></pre>  <p>You will see and be able to monitor your Nextflow jobs in Tower.</p> <p>To configure and execute Nextflow jobs in Cloud environments, visit the Compute environments section.</p>","title":"Via Nextflow run command"},{"location":"getting-started/usage/#api","text":"<p>To learn more about using the Tower API, visit to the API section in this documentation.</p>","title":"API"},{"location":"getting-started/workspace/","text":"<p>Each user has a unique workspace where they can interact and manage all resources such as workflows, compute environments and credentials.</p>  <p>Tip</p> <p>It is also possible to create multiple workspaces within an organization context and associate each of these workspaces with dedicated teams of users, while providing a fine-grained access control model for each of the teams. Please refer the Orgs and Teams section.</p>  <p>The core components of a workspace are described below.</p>","title":"Overview"},{"location":"getting-started/workspace/#launchpad","text":"<p>The Launchpad offers a streamlined UI for launching and managing workflows along with their associated compute environments and credentials. Using Launchpad, it is possible to create a curated set of workflows (including variations of the same workflow) which are ready to be executed on the associated compute environments, while allowing the user the option to customize the workflow level parameters if that's needed.</p>","title":"Launchpad"},{"location":"getting-started/workspace/#runs","text":"<p>The Runs section is used for monitoring a launched workflow with real-time execution metrics such as number of pending or completed processes. For more information please refer Launch section.</p>","title":"Runs"},{"location":"getting-started/workspace/#actions","text":"<p>It is possible to trigger pipelines based on specific events such as version release on Github or general Tower webhook. For further information please refer the Pipeline Actions section.</p>","title":"Actions"},{"location":"getting-started/workspace/#compute-environments","text":"<p>Tower uses the concept of Compute Environments to define the execution platform where a pipeline will be executed. Tower supports launching of pipelines into a growing number of cloud (Azure, AWS and GCP) and on-premise infrastructures (Slurm, IBM LSF and Grid engine etc). For further information please refer the Compute Environments section.</p>","title":"Compute Environments"},{"location":"getting-started/workspace/#credentials","text":"<p>The Credentials section allows users to setup the access credentials for various platforms (Github, Gitlab and BitBucket) as well as various compute environments such as cloud, Slurm  or Kubernetes credentials etc. Please refer to the Compute Environment and Git Integration sections for instructions regarding your infrastructure. For further information please refer the Credentials section.</p>","title":"Credentials"},{"location":"git/overview/","text":"<p>Nextflow has built-in support for Git and integration with several Git-hosting platforms.</p> <p>A data pipelines may be composed by many assets (source code scripts, deployment settings, dependency descriptors such as Conda or Docker files, documentation etc.).</p> <p>By managing complex data pipelines as Git projects, all assets can be precisely tracked and deployed specifying a Git tag, release or commit id. This, combined with containerization, is key for enabling replicable pipeline executions. It also provides the ability to continuously test and validate pipelines as code evolves over time.</p> <p>The following sections detail how to connect to public and private Git-hosting platforms to Tower:</p> <ul> <li>Public Git repositories</li> <li>Private Git repositories</li> </ul>","title":"Overview"},{"location":"git/private_repositories/","text":"<p>Access to private Git repositories can be managed from the Credentials section, accessible from the Credentials tab.</p> <p>Tower provides support to connect to private repositories from the popular Git hosting platforms GitHub, GitLab, and BitBucket.</p> <p></p>  <p>Note</p> <p>All credentials are securely stored using advanced encryption (AES-256) and never exposed by any Tower API.</p>","title":"Private Git repositories"},{"location":"git/private_repositories/#github","text":"<p>To connect a private GitHub repository you need to enter a Name for the credentials, a Username and a Password or Access token. </p>  <p>Hint</p> <p>It is recommended to use an access token instead of a using your password. Personal access tokens (PATs) are an alternative to using passwords for authentication to GitHub when using APIs. Step-by-step instructions to create a personal access token can be found here.</p>","title":"GitHub"},{"location":"git/private_repositories/#gitlab","text":"<p>To connect a private GitLab repository you need to enter a Name for the credentials, a Username, Password and Access token.</p> <p>A GitLab API access token that can be found in your GitLab account page. Make sure to select the <code>api</code>, <code>read_api</code>, and  <code>read_repository</code> options.</p> <p></p>","title":"GitLab"},{"location":"git/private_repositories/#bitbucket","text":"<p>To connect a private BitBucket repository you need to enter a Name for the credentials, a Username and a BitBucket App password. </p> <p>This step-by-step example shows how to create a BitBucket App password.</p>","title":"Bitbucket"},{"location":"git/private_repositories/#self-hosted-git","text":"<p>It is also possible to specify Git server endpoints for private hosting.</p> <p>These can be specified in a file <code>tower.yaml</code> and must be accessible from the backend and cron container instances.</p> <pre>1\n2\n3\n4\n5\n6\n7\n8\n9</pre><pre><code>tower:\n  scm:\n    providers:\n      my_org_bitbucket:\n        server: \"https://bitbucket.my-org.com\"\n        endpoint: \"&lt;API endpoint if different from the above&gt;\"\n        platform: bitbucketserver\n        user: some_user_name\n        password: password_or_access_token\n</code></pre>   <p>Tip</p> <p>For more details on this configuration, see the Nextflow SCM configuration file for examples. You can first test your connection with a Nextflow execution using the standard SCM file and then convert it to the YAML structure, as shown above, for Tower.</p>","title":"Self-hosted Git"},{"location":"git/public_repositories/","text":"<p>Nextflow data pipelines can pulled remotely from either a public or private Git-hosting solution, including the popular platforms: GitHub, GitLab, BitBucket and Gitea.</p> <p>Launching a publicly hosted Git pipeline simply requires adding the Git repo URL in the pipeline to launch field. </p> <p>Note the revision and version numbers for a given repository are automatically pulled using the Git API. By default, the default <code>main/master</code> branch will be executed.</p>  <p>Tip</p> <p>nf-core is a great resource for public nextflow pipelines.</p>","title":"Public Git repositories"},{"location":"installation/system-deployment/","text":"<p>Tip</p> <p>It is highly recommended to first Sign up and try the hosted version of Tower for free or request a demo for a deployment in your own on-premise or cloud environment.</p>  <p>Nextflow Tower is a web application server based on a microservice oriented architecture and designed to maximize the portability, scalability and security of the application.</p> <p>The application is composed of a variety of modules that can be configured and deployed depending on organization's requirements.</p> <p>All components for the Enterprise release are packaged as Docker container images which are hosted and security validated by the Amazon ECR service. The community version can be accessed via GitHub.</p>","title":"System deployment"},{"location":"installation/system-deployment/#deployment-configurations","text":"<p>Warning</p> <p>To install Nextflow Tower on private infrastructure, you'd need a license key. Please contact us at sales@seqera.io to get your license key.</p>","title":"Deployment configurations"},{"location":"installation/system-deployment/#basic-deployment","text":"<p>The minimal Tower configuration only requires the front-end, backend and database modules.</p> <p>These can be executed as Docker containers or as native services running in the hosting environment. Such a minimal configuration is only suggested for evaluation purposes or for a small number of users.</p>","title":"Basic deployment"},{"location":"installation/system-deployment/#kubernetes-deployment","text":"<p>Kubernetes cluster management is emerging as the technology of choice for the deployment of applications requiring high-availability, scalability and security.</p> <p>Nextflow Tower Enterprise includes configuration manifests for the deployment in the Kubernetes environment.</p> <p>This diagram shows the system architecture for the reference deployment on AWS.</p> <p></p>","title":"Kubernetes deployment"},{"location":"installation/system-deployment/#tower-modules","text":"<p>The application is composed of a number of modules that can be configured and deployed depending on user requirements.</p> <p>All components are packaged as Docker container images which are hosted and security validated by the Amazon ECR service.</p>","title":"Tower Modules"},{"location":"installation/system-deployment/#backend-module","text":"<p>The backend is implemented as a JVM-based application server based on the Micronaut framework which provides a modern and secure backbone for the application server.</p> <p>The backend module requires OpenJDK 8 or later.</p> <p>The backend layer implements the main application logic organised in a service layer, which is then exposed via a REST API and defined via an OpenAPI schema.</p> <p>The backend module uses JPA/Hibernate/JDBC API industry standards to connect the underlying relational database.</p> <p>The backend is designed to run standalone or as multiple replicas for scalability when deployed in high-availability mode.  </p>","title":"Backend module"},{"location":"installation/system-deployment/#frontend-module","text":"<p>The frontend module is composed by Angular 8 application which is served by an Nginx web server.</p> <p>The frontend can be configured to expose the application directly to the user/DMZ via an HTTPS connection or through a load balancer.</p>","title":"Frontend module"},{"location":"installation/system-deployment/#storage","text":"<p>Nextflow Tower requires a relational database as its primary storage.</p> <p>It is suggested to use MySQL 5.6, however, any SQL database compatible with JPA/JDBC industry-standards is supported.</p>","title":"Storage"},{"location":"installation/system-deployment/#caching","text":"<p>Tower provides an optional caching module for configurations requiring high availability.</p> <p>This module requires a Redis 5.0 in-memory database.</p>","title":"Caching"},{"location":"installation/system-deployment/#authentication-module","text":"<p>Nextflow Tower supports enterprise authentication mechanisms such as OAuth and LDAP.</p> <p>Third-party authority providers and custom single-sign-on flow can be developed depending on exact customer requirements.</p>","title":"Authentication module"},{"location":"installation/system-deployment/#cron-scheduler","text":"<p>Tower implements a cron service which takes care of executing periodical activities, such as sending e-mail notifications and cleaning up.</p> <p>The cron service can be configured to run as an embedded backend service or an independent service.</p>","title":"Cron scheduler"},{"location":"launch/advanced/","text":"<p>Advanced launch options allow users to modify the configuration and execution of the pipeline.</p>","title":"Advanced Options"},{"location":"launch/advanced/#nextflow-config-file","text":"<p>The Nextflow config field allows the addition of settings to the Nextflow configuration file.</p> <p>This text should follow the same syntax as the Nextflow configuration file.</p> <p>In the example below, we can modify the manifest section to give the pipeline a name and description which will show up in the Tower monitoring section.</p> <p></p> <p>After changing the name in the manifest, when monitoring the pipeline, the name has been overwritten.</p> <p></p>","title":"Nextflow config file"},{"location":"launch/advanced/#pre-post-run-scripts","text":"<p>It is possible to run custom code, either before and after the execution of the Nextflow script. These fields allow users to enter shell commands.</p>","title":"Pre &amp; post-run scripts"},{"location":"launch/advanced/#pull-latest","text":"<p>Enabling this option ensures Nextflow pulls the latest version from the Git repository. This is equivalent to using the <code>-latest</code> flag.</p> <p></p>","title":"Pull latest"},{"location":"launch/advanced/#main-script","text":"<p>Nextflow will attempt to run the script named <code>main.nf</code> in project repository by default. This can be changed via either the <code>manifest.mainScript</code> option or by providing the script filename to run in this field.</p>","title":"Main script"},{"location":"launch/advanced/#workflow-entry-name","text":"<p>Nextflow DSL2 provides the ability to launch specific named workflows. Enter the name of the workflow to be executed in this field.</p>","title":"Workflow entry name"},{"location":"launch/launch/","text":"<p>The Launch Form can be used fo launching pipelines and for creating pipelines for the Launchpad.</p> <p>Consider launching the nf-core/rnaseq workflow using a Google Cloud compute environment.</p> <p>To launch a pipeline:</p> <p>1. Select the Launch button in the navigation bar.</p> <p>The Launch Form view will appear.</p> <p>2. Select the drop down menu to choose a Compute Environment.  </p>  <p>Warning</p> <p>See the Compute Environment documentation to learn how to create an environment for your preferred executor environment.</p>  <p>3. Enter the repository of the Pipeline to launch. For example https://github.com/nf-core/rnaseq.git.</p> <p>4. A Revision number can be used select different versions of pipeline. The Git default branch (main/master) or <code>manifest.defaultBranch</code> in the Nextflow configuration will be used by default.</p> <p>5. The Work directory specifies the location of the Nextflow work directory. The location associated with the compute environment will be selected by default.</p> <p>6. Enter the name(s) of each of the Nextflow Config profiles followed by the <code>Enter</code> key. See the Nextflow Config profiles documentation for more details.</p> <p>7. Enter any Pipeline parameters in YAML or JSON format. YAML example:</p> <pre>1\n2</pre><pre><code>    reads: 's3://nf-bucket/exome-data/ERR013140_{1,2}.fastq.bz2'  \n    paired_end: true\n</code></pre>  <p>8. Select Launch to begin the pipeline execution.</p>  <p>Tip</p> <p>Nextflow pipelines are simply Git repositories and the location can be any public or private Git-hosting platform. See Git Integration in the Tower docs and Pipeline Sharing in the Nextflow docs for more details.</p>   <p>Warning</p> <p>The credentials associated with the compute environment must be able to access the work directory.</p>   <p>Tip</p> <p>In the configuration, the full path to a bucket must be specified with single-quotes around strings no quotes around booleans or numbers.</p>","title":"Launch Form"},{"location":"launch/launchpad/","text":"<p>Launchpad makes it is easy for any workspace user to launch a pre-configured pipeline.</p> <p></p> <p>A pipeline is a repository containing a Nextflow workflow, a compute environment and pipeline parameters.</p>","title":"Launchpad"},{"location":"launch/launchpad/#pipeline-parameters-form","text":"<p>Launchpad automatically detects the presence of a <code>nextflow_schema.json</code> in the root of the repository and dynamically creates a form where users can easily update the parameters. </p>  <p>Tip</p> <p>The parameter forms view will appear, if the workflow has a Nextflow schema file for the parameters. Please refer the Nextflow Schema guide to learn more about the usa-cases and how to create them.</p>  <p>This makes it trivial for users without any expertise in Nextflow to enter their pipeline parameters and launch.</p> <p></p>","title":"Pipeline Parameters Form"},{"location":"launch/launchpad/#adding-a-new-pipeline","text":"<p>Adding a pipeline to the workspace launchpad is similar to the Launch except instead of launching the pipeline it gets added to the list of pipelines with the pre-saved values of fields such as the pipeline parameters and the revision number.</p>  <p>Tip</p> <p>To create your own customized Nextflow Schema for your pipleine, see the examples of from increasing number of <code>nf-core</code> workflows that have adopted this for example eager and rnaseq. </p>","title":"Adding a New Pipeline"},{"location":"launch/notifications/","text":"","title":"Notifications"},{"location":"launch/notifications/#email-notifications","text":"<p>You can receive email notifications at the completion of a  workflow execution.</p> <p>Select your profile page on the top right of the window and select the Send notification email on workflow completion toggle option at the bottom of the page.</p> <p></p>","title":"Email Notifications"},{"location":"launch/relaunch/","text":"<p>Re-launching pipelines is a great way to quickly troubleshoot or make use of Nextflow's resume functionality and re-launch the same pipeline with different parameters.</p> <p>The Resume option is selected by default when re-launching a new pipeline from the Runs monitoring screen. In short, This option allows for the continuation of a workflow execution using Nextflow resume.</p>  <p>Nextflow resume</p> <p>For a detailed explanation of how the resume option works, please visit Part 1 and Part 2 of the Demystifying Nextflow resume description in the Nextflow blog.</p>","title":"Relaunch"},{"location":"monitoring/aggregate_stats/","text":"","title":"Aggregate stats & load"},{"location":"monitoring/aggregate_stats/#aggregate-stats","text":"<p>The Aggregate stats panel displays a real-time summary of the resources used by the workflow. These include total running time ('wall time'), aggregated CPU time (CPU hours), memory usage (GB hours), data i/o and cost.</p> <p></p> <p>The cost is only based on estimated computation usage and does not currently take into account storage or associated network costs. Tower has a database of costs for all cloud instances of AWS and Google Cloud in all regions and zones.</p>","title":"Aggregate Stats"},{"location":"monitoring/aggregate_stats/#load-and-utilization","text":"<p>As processes are being submitted to the compute environment, the Load monitors how many cores and tasks are currently being used. </p> <p>Utilization is calculated for memory and CPUs. This is the average value across all tasks and is calculated by dividing the memory (or CPUs) usage by the memory (or CPUs) requested.</p> <p></p>","title":"Load and Utilization"},{"location":"monitoring/execution/","text":"<p>Selecting a pipeline on the navigation bar will display the workflow details in the main monitoring panel. The main window contains:</p> <ul> <li>Execution section with command-line, parameters, configuration, and execution logs in real-time.</li> <li>Summary and status section.</li> <li>List of pipeline processes.</li> <li>Aggregated stats and load.</li> <li>Detailed list of individual tasks and metrics.</li> </ul>","title":"Execution details & logs"},{"location":"monitoring/execution/#run-information","text":"<p>This top section is composed of 4 tabs containing details about the Nextflow execution:</p> <p>1. The Nextflow Command line to execute the job.</p> <p>2. Parameters including all parameters given in the arguments and arguments taken from the configuration <code>profiles</code> in the <code>params</code> scope.</p> <p>3. Configuration contains all the information included in the configuration file including parameters.</p> <p>4. The Execution log tab is updated in real time with the logs from the main Nextflow process.</p> <p></p>","title":"Run information"},{"location":"monitoring/overview/","text":"","title":"Overview"},{"location":"monitoring/overview/#navigation-bar","text":"<p>Jobs that have been submitted with Tower can be monitored wherever you have an internet connection. </p> <p>The Runs tab contains all previous jobs executions. Each new or resumed job will be given a random name e.g: <code>grave_williams</code>.</p> <p></p> <p>In the left bar:</p> <ul> <li>Blue are running.</li> <li>Green are successfully executed.</li> <li>Yellow are successfully executed where some tasks failed.</li> <li>Red are jobs where at least one task fully failed.</li> <li>Grey are jobs that where forced to stop during execution.</li> </ul> <p>Selecting a run on the left panel will display the job execution details.</p>","title":"Navigation bar"},{"location":"monitoring/overview/#search","text":"<p>The search box allows searching for workflows by <code>project name</code>, <code>run name</code>, <code>session id</code> or <code>manifest name</code>. Moreover, wildcards can be used to filter the desired workflows such as using asterisks <code>*</code> before and after keyword to filter results.</p> <p></p>","title":"Search"},{"location":"monitoring/processes/","text":"<p>In Nextflow, a process is the basic primitive to execute a block of code. The Processes section shows all processes and the status of the tasks. </p> <p>In the example below, there are four tasks of the fastqc process.</p> <p></p> <p>By selecting a process, the Tasks table is filtered below.</p>","title":"Processes"},{"location":"monitoring/sharing/","text":"<p>To share a pipeline execution with a collaborator, select the Sharing icon from the main monitoring panel.</p> <p></p> <p>Select the Add Collaborator button, add your collaborator's email or Tower login and click Confirm.</p> <p></p> <p>An email with the pipeline URL will be sent to the collaborator.</p> <p></p>  <p>Warning</p> <p>Your collaborator's Tower account email must match the email where you sent the invite.</p>  <p>Once shared, the pipeline execution is visible in the user's navigation panel with the launchers name shown.</p> <p>It is important to ensure your collaborators have permissions to your compute resources to make the most of this feature. For example, information in a cloud bucket such as task logs will only be visible if the collaborator also has access to that bucket.</p>","title":"Sharing pipelines"},{"location":"monitoring/summary/","text":"","title":"Summary & status"},{"location":"monitoring/summary/#general","text":"<p>The General summary displays information on the environment and the job being executed:</p> <ul> <li>Unique workflow run ID</li> <li>Workflow run name</li> <li>Date and time of job submission timestamp</li> <li>Project revision and Git commit ID</li> <li>Nextflow session ID</li> <li>Username of the launcher</li> <li>Work directory path</li> <li>Container image</li> <li>Executor</li> <li>Compute environment details</li> <li>Nextflow version</li> </ul>  <p>Tip</p> <p>Hover over with the mouse to get full details on the compute environment.</p>  <p></p>","title":"General"},{"location":"monitoring/summary/#task-status","text":"<p>The Task status section shows in real time the statuses of your workflow tasks. The panel uses the same colour code as the pipelines in the navigation bar.</p> <p>The exact meaning of each status is dependant on the execution platform. </p> <p></p>","title":"Task status"},{"location":"monitoring/tasks/","text":"","title":"Tasks & metrics"},{"location":"monitoring/tasks/#task-table","text":"<p>The Tasks section shows all the tasks from an execution.</p> <p>You can use the <code>Search</code> bar to filter tasks by process name, tag, hash, status, etc. </p> <p>Selecting a status in status section filters the task table.  E.g. clicking in the CACHED card in the status column.</p> <p></p> <p>Selecting a <code>process</code> in the Processes section above will filter all tasks for that specific process.</p> <p></p> <p>Selecting a task in the task table provides specific information about the task in the Task details dialog. </p> <p></p> <p>The task details dialog has the task information tab and the task Execution log tab.</p>","title":"Task table"},{"location":"monitoring/tasks/#task-information","text":"<p>The task information tab contains the process name and task tag in the title. The tab includes:</p> <ul> <li>Command </li> <li>Status</li> <li>Work directory</li> <li>Environment</li> <li>Execution time</li> <li>Resources requested</li> <li>Resources used</li> </ul> <p></p>","title":"Task information"},{"location":"monitoring/tasks/#execution-log","text":"<p>The Execution log provides a realtime log of the individual task of a Nextflow execution. </p> <p>This can be very helpful for troubleshooting. It is possible to download the log files including <code>stdout</code> and <code>stderr</code> from your compute environment.</p> <p></p>","title":"Execution log"},{"location":"monitoring/tasks/#resource-metrics","text":"<p>This section displays plots with CPU, memory, task duration and I/O usage, grouped by process.</p> <p>These metrics can be used to profile an execution to ensure that the correct amount or resources are being requested for each process.</p> <p></p>  <p>Tip</p> <p>Hover the mouse over the box plots to display more details.</p>","title":"Resource metrics"},{"location":"orgs-and-teams/organizations/","text":"<p>Organizations are the top-level structure and contain Workspaces, Members, Teams and Collaborators. </p>","title":"Organizations"},{"location":"orgs-and-teams/organizations/#new-organization","text":"<p>To create a new Organization:</p> <p>1. Click on the dropdown next to your name and select New organization to open the creation dialog.</p> <p>2. On the dialog, fill in the fields as per your organization. The Name and Full name fields are compulsory.</p>  <p>Warning</p> <p>A valid name for the organization must follow specific pattern. Please refer the UI for further instructions.</p>  <p>3. The rest of the fields such as Description, Location, Website URL and Logo Url are optional.</p> <p>4. Once the details are filled-in, you can access the newly created organization using the organizations page, which lists all of your organizations.</p>  <p>Tip</p> <p>It is possible to change the values of the optional fields either using the Edit option on the organizations page or using the Settings tab within the organization page, provided that you are the Owner of the organization .</p>   <p>Note</p> <p>A list of all the included Members, Teams and Collaborators can be found at the organization page.</p>","title":"New Organization"},{"location":"orgs-and-teams/organizations/#members","text":"<p>Once an organization is created, the user who created the organization is the default owner of that organization. It is also possible to invite/add other members as well.</p> <p>Tower, provides access control for members of an organization by classifying them either as an Owner or a Member. Each organization can have multiple owners and members.</p>  <p>Note</p> <p>Owners have full read/write access to modify members, teams, collaborators and setting within a organization. Members are limited in their actions.</p>","title":"Members"},{"location":"orgs-and-teams/organizations/#create-a-new-member","text":"<p>To add a new member to an organization:</p> <ol> <li>Go to the Members tab of the organization menu</li> <li>Click on Invite member</li> <li>Enter the email ID of user you'd like to add to the organization</li> </ol> <p>An e-mail invitiation will be sent which needs to be accepted by the user, once they accept the invitation, they can switch to the organization (or organization workspace) using their workspace dropdown.</p>","title":"Create a new member"},{"location":"orgs-and-teams/organizations/#collaborators","text":"<p>Collaborators are users who are invited to an organizations workspace, but are not members of that organization. As a result, their access is limited to only within that workspace.</p> <p>New collaborators to an organization's workspace can be added using the Participants. To further read the various available access levels for Participants, please refer the participant roles section.</p>  <p>Note</p> <p>Collaborator can only be added from a workspace. For more information, please refer the workspace management section. </p>","title":"Collaborators"},{"location":"orgs-and-teams/organizations/#teams","text":"<p>Teams allows the organization owners to group members and collaborators together into a single unit and to manage them as a whole.</p>","title":"Teams"},{"location":"orgs-and-teams/organizations/#create-a-new-team","text":"<p>To create a new team within an organization:</p> <ol> <li>Go to the Teams tab of the organization menu</li> <li>Click on New team</li> <li>Enter the Name of team </li> <li>Optionally, add the Description and the team's Avatar</li> <li>For the newly created team, click on View</li> <li>Click on Add team member and type in the name of the organization members or collaborators</li> </ol>","title":"Create a new team"},{"location":"orgs-and-teams/overview/","text":"<p>Nextflow Tower simplifies the development and execution of workflows by providing a centralized interface to manage users and resources while providing ready-to-launch workflows for users.</p> <p>This is achieved through the context of Workspaces. To learn more about Workspaces, please refer the Workspace section. By default, each user has their own private workspace, while organizations can workspaces and manage users through role-based access as members and collaborators</p>","title":"Overview"},{"location":"orgs-and-teams/overview/#organization-resources","text":"<p>Tower allows creation of multiple organizations, each of which can contain multiple workspaces with shared users and resources. This allows any organization to customize and organize the usage of resources while maintaining an access control layer for users associated with a workspace.</p> <p>For further information, please refer the Workspace Management section.</p>","title":"Organization resources"},{"location":"orgs-and-teams/overview/#organization-users","text":"<p>Any user can be added or removed from a particular organization or a workspace and can be allocated a specific access role within that workspace. </p> <p>The Teams feature provides a way for the organizations to group various users and participants together into teams, for example <code>workflow-developers</code> or <code>analysts</code>, and apply access control to all the users within this team as a whole.</p> <p>For further information, please refer the User Management  section.</p>","title":"Organization users"},{"location":"orgs-and-teams/workspace-management/","text":"<p>Organization workspaces builds upon the functionality of a User Workspace section, and adds the ability to fine-tune the access level for any particular member, collaborator or team. This is achieved using the concept of Participants in the organization workspaces. </p>  <p>Note</p> <p>A participant may be a member of the parent organization of that workspace or may be a collaborator only for that workspace within that organization.</p>","title":"Workspace Management"},{"location":"orgs-and-teams/workspace-management/#create-a-new-workspace","text":"<p>To create a new workspace within an organization:</p> <ol> <li>Go to the Workspaces tab of the organization menu.</li> <li>Select Create workspace.</li> <li>Enter the Name and Full name of the workspace.</li> <li>Optionally, add the Description of the workspace.</li> <li>Click on Open for the newly created workspace.</li> </ol>  <p>Tip</p> <p>It is possible to change the values of the optional fields either using the Edit option on the workspace listing for an organization or using the Settings tab within the workspace page, provided that you are the Owner of the workspace. </p>  <p>Apart from the Participants tab, the organization workspace is similar to the user workspace therefore, the concepts of Runs, Pipeline Actions, Compute Environments and Credentials are applicable.</p>","title":"Create a new workspace"},{"location":"orgs-and-teams/workspace-management/#add-a-new-participant","text":"<p>To create a new team within an organization:</p> <ol> <li>Go to the Participants tab of the organization menu.</li> <li>Click on Add participant.</li> <li>Enter the Name of new participant. </li> <li>Optionally, update the role associated with the participant of the organization members or collaborators. For more information on roles, please refer the participant roles section.</li> </ol>  <p>Tip</p> <p>A new workspace participant could be either an existing organization member, collaborator, team or a new user.</p>","title":"Add a new Participant"},{"location":"orgs-and-teams/workspace-management/#participant-roles","text":"<p>Organization owners can assign a role-based access level within an organization workspace to any of the participants in the workspace.</p>  <p>Hint</p> <p>It is also possible to group members and collaborators into teams and apply a role to that team.</p>  <p>There are five roles available for every workspace participant.</p> <ol> <li> <p>Owner: The participant have full permissions on any resources within the workspace, including the workspace settings.</p> </li> <li> <p>Admin: The participant have full permission on the resources associated with the workspace. Therefore they can create/modify/delete Pipelines, Compute environments, Actions and Credentials. They can add/remove users to the workspace, but cannot access the workspace settings.</p> </li> <li> <p>Maintain: The participant can launch pipelines and modify pipeline executions (e.g. can change the pipeline launch compute environments, parameters, pre/post-run scripts and nextflow configuration) and create new pipelines in the Launchpad. The users cannot modify Compute Environments and Credentials.</p> </li> <li> <p>Launch: The participant can launch pipelines and modify the pipeline input/output parameters in the Launchpad. They cannot modify the launch configuration and other resources.</p> </li> <li> <p>View: The participant can view the team pipelines and runs in read-only mode.</p> </li> </ol>","title":"Participant roles"},{"location":"orgs-and-teams/workspace-management/#sharing-monitoring-with-workspace","text":"<p>To allow users executing pipelines from the command-line to share their runs with a given workspace, follow the instructions under Getting Started</p>","title":"Sharing monitoring with workspace"},{"location":"pipeline-actions/pipeline-actions/","text":"<p>Pipeline actions allow launching of pipelines based on events. </p> <p>Tower currently offers support for native GitHub webhooks and a general Tower webhook that can be invoked programmatically. Support for Bitbucket and GitLab are coming soon.</p>","title":"Pipeline Actions"},{"location":"pipeline-actions/pipeline-actions/#github-webhooks","text":"<p>This Pipeline action listens for any changes made in the pipeline repository. When a change occurs, Tower triggers the launch of the pipeline in response.</p> <p>To create a new Pipeline action, select the Actions tab and create a new action.</p> <p>1. Enter a name for your Action.</p> <p>2. Choose GitHub webhook as the event source.</p> <p></p> <p>3. Choose the environment where the pipeline will be executed.</p> <p>4. Choose the pipeline to launch and optionally the revision.</p> <p>5. Choose the working directory, the config profiles, and parameters. Select Create.</p> <p></p>  <p>Note</p> <p>The pipeline action is now setup. When a new commit occurs for the selected repository and revision, an event will be triggered in Tower and the pipeline will be launched.</p>  <p></p>  <p>Awesome!</p> <p>You now have an active Pipeline action always listening to the latest changes in your repository.</p>","title":"Github webhooks"},{"location":"pipeline-actions/pipeline-actions/#tower-launch-hooks","text":"<p>A Tower launch hook creates a custom endpoint URL which can be used to trigger the execution of your pipeline programmatically from any script web service.</p> <p>This Pipeline action listens for any changes made in the pipeline repository. When a change occurs, Tower triggers the launch of the pipeline in response.</p> <p>To create a new Pipeline action, select the Actions tab and create a new action.</p> <p>1. After naming your pipeline action, select Tower launch hook as the event source.</p> <p></p> <p>2. Select the environment to execute your pipeline, the pipeline repository URL, the Revision number, the Work directory, the Config profiles, and the Pipeline parameters, and click Create.</p> <p></p> <p>A Tower launch hook has been created at that endpoint that can be used to programmatically launch the corresponding pipeline. The snippet below shows an example <code>cURL</code> command with the authentication token.  </p> <p></p>  <p>Awesome!</p> <p>You now have created an endpoint to programmatically launch a pipeline.</p>   <p>Tip</p> <p>When you create a Tower launch hook, you also create an access Token to allow submitting executions to Tower.</p>  <p>Access Tokens are accessible on the tokens page and can also be accessed from the navigation menu.</p> <p></p>","title":"Tower launch hooks"},{"location":"pipeline-schema/overview/","text":"","title":"Pipeline Schema"},{"location":"pipeline-schema/overview/#overview","text":"<p>This page will give you a detailed description of what pipeline schema files are, why they are used and to give you an in-depth description of how to build and customize your own Pipeline JSON schema file. </p> <p></p>","title":"Overview"},{"location":"pipeline-schema/overview/#what-is-a-pipeline-schema","text":"<p>In short, the main use of pipeline schema is to describe the structure and validation constraints of your workflow parameters. Schemas in general are used to validate parameters before use to prevent software/pipelines failing in unexpected ways at runtime. </p> <p>You can create a UI for your pipeline parameters using the pipeline schema.</p>","title":"What is a Pipeline schema?"},{"location":"pipeline-schema/overview/#why-do-you-need-a-schema-file-for-your-pipelines-or-software-applications","text":"<p>Pipeline schema file is used to describe the different paraments used by the Nextflow workflow and the input parameters that the pipeline accepts.</p> <p>Nextflow Tower uses this file to automatically generate the pipeline inputs form and validate the user provided parameters in a user friendly way.</p>  <p>Tip</p> <p>You can populate the parameters in the pipeline, by uploading a YAML or a JSON file, in addition to filling it on the UI itself.</p>","title":"Why do you need a schema file for your pipelines or software applications?"},{"location":"pipeline-schema/overview/#how-can-i-build-my-own-pipeline-schema-file-for-my-nextflow-pipelines","text":"<p>The pipeline schema is based on json-schema.org syntax,  therefore it can be written with a simple text edition, even though can difficult for complex pipelines.</p> <p>The nf-core project provides an handy tool that helps writing the schema  file by running the <code>nf-core schema build</code> command in the pipeline root directoty.</p> <p>It collects your pipeline parameters and gives you interactive prompts about any missing or unexpected parameters. If no existing schema is found, it will automatically create a JSON schema file for you.</p> <p>For more information, please follow this link.</p>","title":"How can I build my own Pipeline schema file for my Nextflow pipelines?"},{"location":"pipeline-schema/overview/#how-can-i-individualize-or-edit-the-automatically-created-schema-file","text":"<p>Once your pipeline schema file has been built with the <code>nf-core schema build</code>, the tool can send the schema to the nf-core website so that you can use a graphical interface to organize and fill in the schema. </p> <p></p> <p>The tool also checks the status of your schema on the website and once complete, it saves your changes to the file locally. </p> <p>Furthermore, you will get a Build ID/Schema cache ID as can be seen above, so if for any reason you already ran the <code>nf-core schema build</code> command and forgot to save your changes, you can resume editing your JSON schema file by using [our schema builder] (https://nf-co.re/pipeline_schema_builder). </p> <p>If the tool does not automatically take you to the nf-core website to customize your JSON schema file, please click on [the following link] (https://nf-co.re/pipeline_schema_builder). </p> <p>1. Open the link above.</p> <p>2. Copy the schema code you have received into the box below \"Paste your JSON Schema\" in the \"New Schema\" section. </p> <p>3. Click on \"Submit\". </p> <p></p> <p>You will be automatically redirected to the JSON schema builder website, where you can then add parameters, groups and much more. </p> <p>Once you are done editing the schema file, you can click on \"Finished\". </p> <p></p>","title":"How can I individualize or edit the automatically created schema file?"},{"location":"pipeline-schema/overview/#can-i-use-the-pipeline-schema-builder-for-pipelines-outside-of-nf-core","text":"<p>Yes. The schema builder is a tool created by the nf-core community to make the creation and editing of Pipeline schema files easier for developers. Thus, it can be used to create any kind of pipeline. </p>","title":"Can I use the pipeline schema builder for pipelines outside of nf-core?"},{"location":"pipeline-schema/overview/#what-changes-can-i-make-to-my-schema-file-with-the-pipeline-schema-builder","text":"<p>You can add parameters such as identifiers (e.g. <code>productId</code>), a product name (e.g. <code>productName</code>), a selling cost and tags. Additionally, you can define the properties of the identifiers and add groups. </p> <p>For a more in depth guide on schema files, please follow this and this link.</p> <p>If you click on the \"Help\" button in the pipeline schema builder website, you will also be able to get an in-depth explanation of the possible parameters and tips on how to create a schema that fits your needs. </p>","title":"What changes can I make to my schema file with the pipeline schema builder?"}]}